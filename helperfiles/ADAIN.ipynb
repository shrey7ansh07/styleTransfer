{"cells":[{"cell_type":"markdown","metadata":{"id":"jDNx0rMnC8Rz"},"source":["# Importing"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"93YhZSuXeZUV","outputId":"b8414fff-d25c-47ad-cf91-69e356e1896d"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: unrecognized arguments: # Enable inline plotting in Jupyter Notebooks\n"]}],"source":["import os  # For interacting with the file system\n","import zipfile  # For working with ZIP archives\n","import shutil  # For high-level file operations\n","\n","import torch  # For deep learning\n","import torchvision  # For computer vision tasks\n","import torch.nn as nn  # For neural network components\n","\n","import numpy as np  # For numerical operations\n","import pandas as pd  # For data manipulation and analysis\n","from tqdm import tqdm  # For creating progress bars\n","\n","import torch.nn.functional as F  # For PyTorch's functional interface\n","# from torchvision.datasets.utils import download_url  # For downloading datasets\n","from torchvision.models import vgg19  # For the VGG19 model\n","from torchvision.datasets import ImageFolder  # For the ImageFolder dataset class\n","from torch.utils.data import DataLoader  # For loading data in PyTorch\n","import torchvision.transforms as T  # For data transformations\n","from torch.utils.data import random_split  # For randomly splitting datasets\n","from torchvision.utils import make_grid  # For creating grid images\n","\n","import PIL  # For image processing\n","from PIL import Image  # For working with images\n","import random  # For generating random numbers\n","import matplotlib # For Plotting\n","import matplotlib.pyplot as plt  # For plotting\n","import matplotlib.image as mpimg  # For working with images\n","%matplotlib inline  # Enable inline plotting in Jupyter Notebooks\n","\n","# Set the background color of the figures to white\n","matplotlib.rcParams['figure.facecolor'] = '#ffffff'\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPA1zS6gN7WU","outputId":"81d4f8ab-efed-4a52-e64a-9ff1dd43fc3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in c:\\users\\yashs\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.8.0)\n"]}],"source":["!pip install torchinfo\n","from torchinfo import summary"]},{"cell_type":"markdown","metadata":{"id":"YMJGtL3bhH70"},"source":["# Data Download"]},{"cell_type":"markdown","metadata":{"id":"u50YZVMs1IJa"},"source":["## Style data download"]},{"cell_type":"markdown","metadata":{"id":"I4vhvg99lPHM"},"source":["Downloading the [Painters by numbers](https://www.kaggle.com/competitions/painter-by-numbers/data) train dataset from WIKIArt using the Kaggle API.\n","(Ignore the error as there was a mistake in the file path )"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ht0rfR60A0Ys","outputId":"5189035a-f111-4803-fa63-370b351e2551"},"outputs":[{"name":"stderr","output_type":"stream","text":["'ls' is not recognized as an internal or external command,\n","operable program or batch file.\n","The syntax of the command is incorrect.\n","'cp' is not recognized as an internal or external command,\n","operable program or batch file.\n","'chmod' is not recognized as an internal or external command,\n","operable program or batch file.\n","Traceback (most recent call last):\n","  File \"C:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n","    return _run_code(code, main_globals, None,\n","  File \"C:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n","    exec(code, run_globals)\n","  File \"C:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\Scripts\\kaggle.exe\\__main__.py\", line 4, in <module>\n","  File \"C:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\__init__.py\", line 7, in <module>\n","    api.authenticate()\n","  File \"C:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\kaggle\\api\\kaggle_api_extended.py\", line 407, in authenticate\n","    raise IOError('Could not find {}. Make sure it\\'s located in'\n","OSError: Could not find kaggle.json. Make sure it's located in C:\\Users\\yashs\\.kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/ec2-user/SageMaker/train.zip'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\styleTransfer_yash\\helperfiles\\ADAIN.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/styleTransfer_yash/helperfiles/ADAIN.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m local_zip \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/ec2-user/SageMaker/train.zip\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/styleTransfer_yash/helperfiles/ADAIN.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Open the ZIP file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/styleTransfer_yash/helperfiles/ADAIN.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m zip_ref \u001b[39m=\u001b[39m zipfile\u001b[39m.\u001b[39;49mZipFile(local_zip, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/styleTransfer_yash/helperfiles/ADAIN.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Create the directory '/home/ec2-user/SageMaker/style-data' if it doesn't exist\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/styleTransfer_yash/helperfiles/ADAIN.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mmkdir /home/ec2-user/SageMaker/style-data\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[1;32mc:\\Users\\yashs\\AppData\\Local\\Programs\\Python\\Python310\\lib\\zipfile.py:1240\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m   1239\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1240\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(file, filemode)\n\u001b[0;32m   1241\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m   1242\u001b[0m         \u001b[39mif\u001b[39;00m filemode \u001b[39min\u001b[39;00m modeDict:\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ec2-user/SageMaker/train.zip'"]}],"source":["# Check the details of the 'kaggle.json' file\n","!ls -lha /home/ec2-user/SageMaker/kaggle.json\n","\n","# Install the 'kaggle' package\n","!pip install -q kaggle\n","\n","# Create the directory '~/.kaggle' if it doesn't exist\n","!mkdir -p ~/.kaggle\n","\n","# Copy the 'kaggle.json' file to the '~/.kaggle' directory\n","!cp kaggle.json ~/.kaggle/\n","\n","# Set the appropriate permissions for the 'kaggle.json' file\n","!chmod 600 /home/ec2-user/SageMaker/kaggle.json\n","\n","# Download the 'train.zip' file from the 'painter-by-numbers' competition\n","!kaggle competitions download -f train.zip -p '/home/ec2-user/SageMaker' -o painter-by-numbers\n","\n","# Set the path to the downloaded ZIP file\n","local_zip = '/home/ec2-user/SageMaker/train.zip'\n","\n","# Open the ZIP file\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","\n","# Create the directory '/home/ec2-user/SageMaker/style-data' if it doesn't exist\n","!mkdir /home/ec2-user/SageMaker/style-data\n","\n","# Extract the contents of the ZIP file to the '/home/ec2-user/SageMaker/style-data' directory\n","zip_ref.extractall('/home/ec2-user/SageMaker/style-data')\n","\n","# Close the ZIP file\n","zip_ref.close()\n","\n","# Remove the downloaded ZIP file\n","os.remove(local_zip)\n","\n","# Count the number of images in the '/home/ec2-user/SageMaker/train' directory and print the count\n","print('The number of images present in the WikiArt dataset are:', len(os.listdir('/home/ec2-user/SageMaker/train')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03SAzudBlERE","outputId":"1d726312-2f69-452e-ceca-a051b2fe5f68"},"outputs":[],"source":["print('The number of images present in WikiArt dataset are:',len(os.listdir('/home/ec2-user/SageMaker/style-data/train')))"]},{"cell_type":"markdown","metadata":{"id":"TH38ltW51Uex"},"source":["## Content data download"]},{"cell_type":"markdown","metadata":{"id":"7Zt_X6C1ltxF"},"source":["Downloading the COCO 2014 dataset from [COCOdataset.org](https://cocodataset.org/#download)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-1t_--nWR0o","outputId":"66a5c6e8-e122-4dbd-a031-360a9e15c423"},"outputs":[],"source":["# Download the COCO dataset ZIP file\n","!wget --no-check-certificate \\\n","    \"http://images.cocodataset.org/zips/train2014.zip\" \\\n","    -O \"/home/ec2-user/SageMaker/coco.zip\"\n","\n","# Set the path to the downloaded ZIP file\n","local_zip = '/home/ec2-user/SageMaker/coco.zip'\n","\n","# Open the ZIP file\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","\n","# Create the directory '/home/ec2-user/SageMaker/content-data' if it doesn't exist\n","!mkdir /home/ec2-user/SageMaker/content-data\n","\n","# Extract the contents of the ZIP file to the '/home/ec2-user/SageMaker/content-data' directory\n","zip_ref.extractall('/home/ec2-user/SageMaker/content-data')\n","\n","# Close the ZIP file\n","zip_ref.close()\n","\n","# Remove the downloaded ZIP file\n","os.remove(local_zip)\n","\n","# Count the number of images in the '/home/ec2-user/SageMaker/content-data/train2014' directory and print the count\n","print('The number of images present in the COCO dataset are:', len(os.listdir('/home/ec2-user/SageMaker/content-data/train2014')))\n"]},{"cell_type":"markdown","metadata":{"id":"umzGZPtap6QL"},"source":["## Downloading additional data for the Style Dataset."]},{"cell_type":"markdown","metadata":{"id":"BF7gD4rhm7sM"},"source":["In order to address the discrepancy in the number of images between the COCO and WIKIArt datasets, I have downloaded additional images from the test file of the WIKIArt dataset and moved them to the training dataset. This approach was taken to increase the size of the training dataset and improve the overall performance of the model.\n","\n","By doing so, the model will be exposed to a more diverse set of images, which should help it better generalize to new, unseen data. Additionally, the increased number of images will allow the model to learn more robust features, which should also lead to better performance.\n","\n","It is important to note that this process was undertaken with careful consideration. The additional images were selected based on their relevance to the task at hand and were thoroughly vetted to ensure that they were appropriate for use in the training dataset.\n","\n","Overall, this approach represents a promising strategy for improving the performance of the model and enhancing its ability to accurately classify and identify images from the COCO and WIKIArt datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rKgN-KwKlERH","outputId":"6485108f-b21b-4aad-9604-7908b4d0d927"},"outputs":[],"source":["# Set the path to the ZIP file\n","local_zip = '/home/ec2-user/SageMaker/IMG.zip'\n","\n","# Open the ZIP file\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","\n","# Extract the contents of the ZIP file to the '/home/ec2-user/SageMaker/content-data-extra' directory\n","zip_ref.extractall('/home/ec2-user/SageMaker/content-data-extra')\n","\n","# Close the ZIP file\n","zip_ref.close()\n","\n","# Count the number of images in the '/home/ec2-user/SageMaker/content-data-extra/IMG' directory and print the count\n","print('The number of images present in the COCO dataset are:', len(os.listdir('/home/ec2-user/SageMaker/content-data-extra/IMG')))\n"]},{"cell_type":"markdown","metadata":{"id":"AD901uWFpoFC"},"source":["I deleted 5 images randomly manually from the folder."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAwAz0cAlERH","outputId":"fa1e2ccf-1122-4163-bbe8-dafbad4c5032"},"outputs":[],"source":["print('The number of images present in COCO dataset are:',len(os.listdir('/home/ec2-user/SageMaker/content-data-extra/IMG')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6OGVrimDlERJ"},"outputs":[],"source":["source = '/home/ec2-user/SageMaker/content-data-extra/IMG'\n","src_folder = '/home/ec2-user/SageMaker/content-data-extra/IMG'\n","dst_folder = '/home/ec2-user/SageMaker/style-data/train'\n","\n","# gather all files\n","allfiles = os.listdir(source)\n","\n","# iterate on all files to move them to destination folder\n","for i in allfiles:\n","    # Construct the full path to the source file\n","    src_path = os.path.join(src_folder, i)\n","\n","    # Construct the full path to the destination file\n","    dst_path = os.path.join(dst_folder, i+'-moved')\n","    shutil.move(src_path, dst_path)"]},{"cell_type":"markdown","metadata":{"id":"l1RTba8rpwRD"},"source":["As we can see the no. of images in both the training set are equal now."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bouC56Cqsls-","outputId":"804ff18c-ea6e-4949-9c2f-1cc9ebb3cf6e"},"outputs":[],"source":["print('The number of images present in WIKIART dataset are:',len(os.listdir('/home/ec2-user/SageMaker/style-data/train')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psHPuXsblERJ","outputId":"2f372f80-47b0-4624-a2e5-6ad2a3375f50"},"outputs":[],"source":["print('The number of images present in COCO dataset are:',len(os.listdir('/home/ec2-user/SageMaker/content-data/train2014')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjGqxSrRoRv3"},"outputs":[],"source":["# Creating new folders for storing testing dataset.\n","!mkdir /home/ec2-user/SageMaker/test-content\n","!mkdir /home/ec2-user/SageMaker/test-style\n","!mkdir /home/ec2-user/SageMaker/test-content/test\n","!mkdir /home/ec2-user/SageMaker/test-style/test"]},{"cell_type":"markdown","metadata":{"id":"am1z1Lxf1ZDy"},"source":["## Visualizing the dataset"]},{"cell_type":"markdown","metadata":{"id":"ewp0Ao9NrABE"},"source":["Every time the 'show_images' function is run, it plots a grid of 20 different images randomly selected from the provided dataset. This allows for a diverse set of images to be displayed with each run, making it useful for exploring the dataset and identifying any potential issues or anomalies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4sEr5ZIeZYV"},"outputs":[],"source":["def show_images(dataset):\n","    \"\"\"\n","    Display a grid of images from the given dataset.\n","\n","    Args:\n","        dataset (str): Path to the directory containing the images.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Parameters for our graph; we'll output images in a 5x4 configuration\n","    nrows = 5\n","    ncols = 4\n","\n","    fig = plt.gcf()  # Get the current figure\n","    fig.set_size_inches(ncols * 5, nrows * 5)  # Set the figure size\n","\n","    for i in range(20):\n","        # Set up subplot; subplot indices start at 1\n","        img = mpimg.imread(os.path.join(dataset, random.choice(os.listdir(dataset))))\n","        sp = plt.subplot(nrows, ncols, i + 1)  # Create a subplot\n","        sp.axis('Off')  # Don't show axes (or gridlines)\n","        plt.imshow(img)  # Display the image\n","\n","    # No need to return anything as the function only displays the images\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Qy8ytHUjXfhn","outputId":"8a01388b-9246-434b-8737-8f67dda68034"},"outputs":[],"source":["show_images('/home/ec2-user/SageMaker/content-data/train2014')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7ES42lZKDrBv","outputId":"8a7230f4-f0bf-4abb-e086-08c65828e49a"},"outputs":[],"source":["show_images('/home/ec2-user/SageMaker/style-data/train')"]},{"cell_type":"markdown","metadata":{"id":"xMsrqf4O5J21"},"source":["# Data Transformation and Normalization Pipeline for Image Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K_fJh0t_eZcA"},"outputs":[],"source":["# Define the statistics used for normalization\n","stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","\n","# Define the normalization transform\n","normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","# Define the batch size\n","batch_size = 8\n","\n","# Define the data transformations for training\n","tfms = T.Compose([\n","    T.Resize((512, 512)),  # Resize the image to (512, 512)\n","    T.RandomCrop((256, 256)),  # Randomly crop the image to (256, 256)\n","    T.ToTensor(),  # Convert the image to a tensor\n","    T.Normalize(*stats, inplace=True)  # Normalize the image with the defined statistics\n","])\n","\n","# Define the data transformations for testing\n","test_tfms = T.Compose([\n","    T.Resize((512, 512)),  # Resize the image to (512, 512)\n","    T.ToTensor(),  # Convert the image to a tensor\n","    T.Normalize(*stats, inplace=True)  # Normalize the image with the defined statistics\n","])\n"]},{"cell_type":"markdown","metadata":{"id":"l-obcRKEroS5"},"source":["## Data Loading and Processing Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVzH1yKzeaG4"},"outputs":[],"source":["content_dataset = ImageFolder('/home/ec2-user/SageMaker/content-data', tfms)    # Load the content dataset from the specified directory using the given transformations\n","style_dataset = ImageFolder('/home/ec2-user/SageMaker/style-data', tfms)        # Load the style dataset from the specified directory using the given transformations\n","\n","test_content_dataset = ImageFolder('/home/ec2-user/SageMaker/test-content', test_tfms)       # Load the test content dataset from the specified directory using the given transformations\n","test_style_dataset = ImageFolder('/home/ec2-user/SageMaker/test-style', test_tfms)           # Load the test style dataset from the specified directory using the given transformations\n","\n","content_dl = DataLoader(content_dataset, batch_size=batch_size, shuffle=True, num_workers=2,drop_last=True)      # Create a data loader for the content dataset with the specified batch size, shuffling, and number of workers\n","style_dl = DataLoader(style_dataset, batch_size=batch_size, shuffle=True, num_workers=2,drop_last=True)          # Create a data loader for the style dataset with the specified batch size, shuffling, and number of workers\n","\n","test_content_dl = DataLoader(test_content_dataset, batch_size=1, num_workers=2)  # Create a data loader for the test content dataset with a batch size of 1 and 2 workers\n","test_style_dl = DataLoader(test_style_dataset, batch_size=1, num_workers=2)      # Create a data loader for the test style dataset with a batch size of 1 and 2 workers"]},{"cell_type":"markdown","metadata":{"id":"M2ToxmLPsGNA"},"source":["The denormalize function and show_batch function are designed to visualize the images in a batch after being processed by the normalization pipeline defined earlier.\n","\n","The denormalize function takes as input a batch of normalized images, along with the means and standard deviations used for normalization, and returns the corresponding denormalized images. This is useful for visualizing the images in the pipline after processing.\n","\n","The show_batch function displays a batch of images using matplotlib and the make_grid function from the torchvision.utils module. It uses the denormalize function to convert the normalized images back to their original scale before displaying them.\n","\n","The function creates a new figure with the specified size and removes the axis ticks to produce a cleaner visual display. The break statement is used to ensure that only one batch of data is displayed, making it easier to review the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRL8gidO5RlS"},"outputs":[],"source":["def denormalize(images, means, stds):\n","    \"\"\"\n","    Denormalize the images using the provided mean and standard deviation values.\n","\n","    Args:\n","        images (Tensor): Input tensor of images.\n","        means (tuple): Mean values used for normalization.\n","        stds (tuple): Standard deviation values used for normalization.\n","\n","    Returns:\n","        Tensor: Denormalized images.\n","    \"\"\"\n","    means = torch.tensor(means).reshape(1, 3, 1, 1)  # Reshape means to match the shape of the input images\n","    stds = torch.tensor(stds).reshape(1, 3, 1, 1)  # Reshape stds to match the shape of the input images\n","    return images * stds + means  # Denormalize the images using element-wise multiplication and addition\n","\n","def show_batch(dl):\n","    \"\"\"\n","    Display a batch of images from the given data loader.\n","\n","    Args:\n","        dl (DataLoader): Data loader containing the batch of images and labels.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Iterate over the data loader to get the batch of images and labels\n","    for images, labels in dl:\n","        # Create a new figure and axes with a large size for displaying the images\n","        fig, ax = plt.subplots(figsize=(30, 30))\n","\n","        # Remove the ticks on the x and y axes\n","        ax.set_xticks([]); ax.set_yticks([])\n","\n","        # Denormalize the images using the provided stats\n","        denorm_images = denormalize(images, *stats)\n","\n","        # Create a grid of images and display it using the axes\n","        # Permute the dimensions of the tensor to match the expected format (H, W, C)\n","        # Clamp the values between 0 and 1 to ensure valid image display\n","        ax.imshow(make_grid(denorm_images, nrow=8).permute(1, 2, 0).clamp(0, 1))\n","\n","        # Exit the loop after processing the first batch\n","        break\n","\n","        # No need to return anything as the function only displays the images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"id":"0pvOfK3C5Rhm","outputId":"c9b7e33e-f116-40cb-b68a-d7485745708b"},"outputs":[],"source":["show_batch(content_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"id":"1evxSU9Lc4No","outputId":"4ae403c8-c071-4e2b-a1cf-1782df5a4183"},"outputs":[],"source":["show_batch(style_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qA9-i6KsIOIc","outputId":"6f18edf1-64fc-4af2-c66f-b6a8553abab1"},"outputs":[],"source":["show_batch(test_content_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UAErWvu6ITxQ","outputId":"9e02d61b-d7a2-4b7e-9236-3c6bd0079b3b"},"outputs":[],"source":["show_batch(test_style_dl)"]},{"cell_type":"markdown","metadata":{"id":"vv4nmWcbtZjA"},"source":["###Device and Data Loader Utility Functions for Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUR3pIoi5RcL"},"outputs":[],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to chosen device\"\"\"\n","    if isinstance(data, (list,tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DeviceDataLoader():\n","    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","\n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Number of batches\"\"\"\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n82_qzyQ5XHl","outputId":"9eda9af4-def1-4292-800f-133213ab9b2b"},"outputs":[],"source":["device = get_default_device()\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f74v5ATP5XFS"},"outputs":[],"source":["content_dl = DeviceDataLoader(content_dl, device)\n","style_dl = DeviceDataLoader(style_dl, device)\n","\n","test_content_dl = DeviceDataLoader(test_content_dl, device)\n","test_style_dl = DeviceDataLoader(test_style_dl, device)"]},{"cell_type":"markdown","metadata":{"id":"Oij2qccic98r"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["747ed4e30ae94c40aa217228430c0cbc","3e8b0d63c57a4f3f825ea905f962df8d","b16ebe22c0c147e983324cf354dbe730","b52bea844f0644c7aa760a9856d458fe","02e38fc8863f4a07b9c5c5f088cf50ea","3bcb03f09d124ccc9fdaf56fa31b2fb6","6b3b21181c7447cbbf8e59a40af56ee7","c5c7496780814ad5aba072790591e1bd","6ece4908ecff4bf9ab54a48e156db713","49321942e186447fb8ef3b807c996d75","6e56006fdbbd40b2a2ec2ebbaae9fd11","be46480219e441b1a5f7154d389b27b0"]},"id":"4VnFARcS5W7i","outputId":"79aea297-5261-459b-8be5-9b7479484694"},"outputs":[],"source":["vg19 = vgg19(True)\n","print(vg19)"]},{"cell_type":"markdown","metadata":{"id":"cJV5MWgEZvIZ"},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfD4nPszX7ZY"},"outputs":[],"source":["class VGGEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Load the VGG19 model with default weights\n","        vgg = vgg19(weights='DEFAULT').features\n","\n","        # Define different slices of the VGG model for feature extraction\n","        self.slice1 = vgg[:2]\n","        self.slice2 = vgg[2:7]\n","        self.slice3 = vgg[7:12]\n","        self.slice4 = vgg[12:21]\n","\n","        # Set requires_grad=False for all parameters to freeze the pre-trained weights\n","        for p in self.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, images, output_last_feature=False):\n","        \"\"\"\n","        Forward pass of the VGGEncoder.\n","\n","        Args:\n","            images (Tensor): Input images to be encoded.\n","            output_last_feature (bool): If True, only the last feature is returned. Otherwise, all intermediate features are returned.\n","\n","        Returns:\n","            Tensor or Tuple[Tensor]: Encoded features from the VGG encoder. If output_last_feature is True, returns the last feature tensor. Otherwise, returns a tuple of feature tensors from each slice.\n","        \"\"\"\n","        # Pass the input images through each slice of the VGG encoder\n","        h1 = self.slice1(images)\n","        h2 = self.slice2(h1)\n","        h3 = self.slice3(h2)\n","        h4 = self.slice4(h3)\n","\n","        if output_last_feature:\n","            # Return the last feature tensor\n","            return h4\n","        else:\n","            # Return a tuple of feature tensors from each slice\n","            return h1, h2, h3, h4\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["e3415a1beea34a278f6c86ce6b9780b1"]},"id":"3RXuNSWWCnLA","outputId":"af3449f8-1bb4-406b-c6dd-ac1c1ca48570"},"outputs":[],"source":["enc = VGGEncoder()\n","enc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A1pAQo3s9v8T","outputId":"9da1c3a3-2b58-463b-c40b-de413dee453b"},"outputs":[],"source":["summary(enc, input_size=(batch_size, 3, 512, 512))"]},{"cell_type":"markdown","metadata":{"id":"kE09DmW57_sN"},"source":["## ADAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPCBhPwg7-8p"},"outputs":[],"source":["def calc_mean_std(features):\n","    \"\"\"\n","    Calculate the mean and standard deviation of the input features.\n","\n","    Args:\n","        features (Tensor): Input features of shape [batch_size, c, h, w].\n","\n","    Returns:\n","        features_mean (Tensor): Mean of the features of shape [batch_size, c, 1, 1].\n","        features_std (Tensor): Standard deviation of the features of shape [batch_size, c, 1, 1].\n","    \"\"\"\n","\n","    # Get the batch size and number of channels from the input features\n","    batch_size, c = features.size()[:2]\n","\n","    # Calculate the mean and reshape it to match the required shape\n","    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n","\n","    # Calculate the standard deviation and reshape it to match the required shape\n","    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n","\n","    return features_mean, features_std\n","\n","\n","def adain(content_features, style_features):\n","    \"\"\"\n","    Apply Adaptive Instance Normalization (AdaIN) to the content features using style features.\n","\n","    Args:\n","        content_features (Tensor): Content features of shape [batch_size, c, h, w].\n","        style_features (Tensor): Style features of shape [batch_size, c, h, w].\n","\n","    Returns:\n","        normalized_features (Tensor): Normalized features of shape [batch_size, c, h, w].\n","    \"\"\"\n","\n","    # Calculate the mean and standard deviation of the content and style features\n","    content_mean, content_std = calc_mean_std(content_features)\n","    style_mean, style_std = calc_mean_std(style_features)\n","\n","    # Normalize the content features using the style features\n","    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean    # Adaptive Instance Normalization\n","\n","    return normalized_features\n"]},{"cell_type":"markdown","metadata":{"id":"a7__nlcsZ0lP"},"source":["## Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr1CKQnpZ0Jn"},"outputs":[],"source":["class RC(torch.nn.Module):\n","    \"\"\"\n","    A wrapper of ReflectionPad2d and Conv2d\n","\n","    This class represents a combination of reflection padding and a convolutional layer.\n","    It applies reflection padding to the input and then performs convolution on the padded input.\n","    Optionally, it applies ReLU activation to the output of the convolution.\n","\n","    Args:\n","        in_channels (int): Number of input channels.\n","        out_channels (int): Number of output channels.\n","        kernel_size (int): Size of the convolution kernel. Default is 3.\n","        pad_size (int): Size of the reflection padding. Default is 1.\n","        activated (bool): Whether to apply activation (ReLU) after convolution. Default is True.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n","        super().__init__()\n","        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n","        self.activated = activated\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the RC module.\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, in_channels, height, width).\n","\n","        Returns:\n","            Output tensor  after applying reflection padding, convolution,\n","            and activation (if enabled) of shape (batch_size, out_channels, height, width)\n","        \"\"\"\n","        h = self.pad(x)     # Apply reflection padding to the input tensor\n","        h = self.conv(h)    # Perform convolution on the padded input\n","        if self.activated:  # Apply ReLU activation if activated is True\n","            return F.relu(h)\n","        else:\n","            return h         # Otherwise, return the output without activation\n","\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Decoder network for image reconstruction.\n","    This network takes features extracted by an encoder network with\n","    adaptive instance normalization applied using style features and generates a reconstructed image.\n","    This module consists of a series of RC (ReflectionPad2d and Conv2d) layers for upsampling and Image reconstruction.\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.rc1 = RC(512, 256, 3, 1)\n","        self.rc2 = RC(256, 256, 3, 1)\n","        self.rc3 = RC(256, 256, 3, 1)\n","        self.rc4 = RC(256, 256, 3, 1)\n","        self.rc5 = RC(256, 128, 3, 1)\n","        self.rc6 = RC(128, 128, 3, 1)\n","        self.rc7 = RC(128, 64, 3, 1)\n","        self.rc8 = RC(64, 64, 3, 1)\n","        self.rc9 = RC(64, 3, 3, 1, False)\n","\n","    def forward(self, features):\n","       \"\"\"\n","        Forward pass of the Decoder module.\n","\n","        Args:\n","            features (torch.Tensor): Input features from the encoder module.\n","\n","        Returns:\n","            torch.Tensor: Output tensor representing the reconstructed image.\n","        \"\"\"\n","        # Forward pass of the Decoder module for image upsampling and reconstruction\n","        h = self.rc1(features)\n","        h = F.interpolate(h, scale_factor=2)      # Perform upsampling using F.interpolate with scale factor 2\n","        h = self.rc2(h)\n","        h = self.rc3(h)\n","        h = self.rc4(h)\n","        h = self.rc5(h)\n","        h = F.interpolate(h, scale_factor=2)      # Perform another upsampling using F.interpolate with scale factor 2\n","        h = self.rc6(h)\n","        h = self.rc7(h)\n","        h = F.interpolate(h, scale_factor=2)      # Perform another upsampling using F.interpolate with scale factor 2\n","        h = self.rc8(h)\n","        h = self.rc9(h)\n","        return h"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6p5dI2F9xzy","outputId":"fbb21ce2-84b5-4cb4-82b1-2bd531b3db3d"},"outputs":[],"source":["dec = Decoder()\n","dec"]},{"cell_type":"markdown","metadata":{"id":"__0AxR5K9jYS"},"source":["# Computing Loss and Image Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fS73WtejX7Ws"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg_encoder = VGGEncoder()  # Initialize the VGGEncoder to extract content and style features\n","        self.decoder = Decoder()        # Initialize the Decoder for image reconstruction\n","\n","    def generate(self, content_images, style_images, alpha=1.0):\n","        \"\"\"\n","        Generate a stylized image by  feature maps of the content image\n","        are normalized using the mean and standard deviation of the corresponding\n","        style image's feature maps. combining content and style features.\n","        The normalized feature maps are used to reconstruct the image.\n","\n","        Args:\n","            content_images (torch.Tensor): Input content images as tensors.\n","            style_images (torch.Tensor): Input style images as tensors.\n","            alpha (float, optional):  Control the level of stylization. Default is 1.0.\n","\n","        Returns:\n","            torch.Tensor: Stylized output image.\n","        \"\"\"\n","        content_features = self.vgg_encoder(content_images, output_last_feature=True)  # Extract content features\n","        style_features = self.vgg_encoder(style_images, output_last_feature=True)      # Extract style features\n","        t = adain(content_features, style_features)    # Apply Adaptive Instance Normalization (AdaIN) to normalize features\n","        t = alpha * t + (1 - alpha) * content_features  # Blend content and style features based on alpha\n","        out = self.decoder(t)                          # Generate the stylized output using the decoder\n","        return out\n","\n","    @staticmethod\n","    def calc_content_loss(out_features, t):\n","        \"\"\"\n","        Calculate the content loss between output features and target features.\n","\n","        Args:\n","            out_features (torch.Tensor): Output features generated by the model.\n","            t (torch.Tensor): Target features (content features).\n","\n","        Returns:\n","            torch.Tensor: Content loss value.\n","        \"\"\"\n","        return F.mse_loss(out_features, t)  # Calculate Mean Squared Error (MSE) loss\n","\n","    @staticmethod\n","    def calc_style_loss(content_middle_features, style_middle_features):\n","        \"\"\"\n","        Calculate the style loss between content and style features.\n","\n","        Args:\n","            content_middle_features (list of torch.Tensor): Content features at different layers.\n","            style_middle_features (list of torch.Tensor): Style features at different layers.\n","\n","        Returns:\n","            torch.Tensor: Style loss value.\n","        \"\"\"\n","        loss = 0\n","        for c, s in zip(content_middle_features, style_middle_features):\n","            c_mean, c_std = calc_mean_std(c)   # Calculate mean and standard deviation of content features\n","            s_mean, s_std = calc_mean_std(s)   # Calculate mean and standard deviation of style features\n","            loss += F.mse_loss(c_mean, s_mean) + F.mse_loss(c_std, s_std)  # Calculate MSE loss between means and stds\n","        return loss\n","\n","    def forward(self, content_images, style_images, alpha=1.0, lam=10):\n","        \"\"\"\n","        Forward pass of the Model.\n","\n","        Args:\n","            content_images (torch.Tensor): Input content images as tensors.\n","            style_images (torch.Tensor): Input style images as tensors.\n","            alpha (float, optional): Style strength factor. Default is 1.0.\n","            lam (float, optional): Weight of the style loss. Default is 10.\n","\n","        Returns:\n","            torch.Tensor: Total loss value.\n","        \"\"\"\n","        content_features = self.vgg_encoder(content_images, output_last_feature=True)  # Extract features from the content image\n","        style_features = self.vgg_encoder(style_images, output_last_feature=True)      # Extract features from the style image.\n","        t = adain(content_features, style_features)    # Apply Adaptive Instance Normalization (AdaIN) to combine features\n","        t = alpha * t + (1 - alpha) * content_features  # Blend content and style features based on alpha\n","        out = self.decoder(t)                          # Generate the stylized output using the decoder\n","\n","        output_features = self.vgg_encoder(out, output_last_feature=True)    # Extract features from the stylized output\n","        output_middle_features = self.vgg_encoder(out, output_last_feature=False)  # Extract middle-level features from the stylized output\n","        style_middle_features = self.vgg_encoder(style_images, output_last_feature=False)  # Extract style middle features from the style image\n","\n","        loss_c = self.calc_content_loss(output_features, t)     # Calculate content loss\n","        loss_s = self.calc_style_loss(output_middle_features, style_middle_features)  # Calculate style loss from the middle-level features from the stylized output and the style image\n","        loss = loss_c + lam * loss_s  # Combine content and style loss with the specified lambda weight\n","        return loss\n"]},{"cell_type":"markdown","metadata":{"id":"MKC6xEE7U0ry"},"source":["# Saving Samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SAUpmW7wVN0N"},"outputs":[],"source":["def denorm(tensor, device):\n","    \"\"\"\n","    Denormalizes the image tensor using the mean and standard deviation values of ImageNet.\n","\n","    Args:\n","        tensor (torch.Tensor): The input tensor to be denormalized. It should have shape (C, H, W).\n","        device (str or torch.device): The device on which the computations should be performed.\n","\n","    Returns:\n","        torch.Tensor: The denormalized tensor with values clamped between 0 and 1.\n","    \"\"\"\n","    # Define the standard deviation values for each channel (R, G, B)\n","    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n","\n","    # Define the mean values for each channel (R, G, B)\n","    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n","\n","    # Perform denormalization and clamp the tensors between value of 0 and 1 by applying the following formula:\n","    # Denorm = (Input * STD) + Mean\n","    denormalized_tensor = torch.clamp(tensor * std + mean, 0, 1)\n","\n","    return denormalized_tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4u3pbJKulERR"},"outputs":[],"source":["from torchvision.transforms.functional import resize as rez\n","\n","new_height, new_width = 512, 512"]},{"cell_type":"markdown","metadata":{"id":"wiSIIWOdw3xv"},"source":["The above code defines a function called save_sample which is used to generate and save sample images during the training process. The function takes the current epoch and iteration as input parameters and uses them to create a unique file name for each saved image.\n","\n","Inside the function, the model is used to generate output images from a pair of style and content images in the test dataset. The style, content and output images are first denormalized and all of them are resized to be of the exact same size that is (512,512) for concatenation purpose. The resulting images are then saved as a single image file using the save_image function provided by the torchvision library.\n","\n","This function is useful for visualizing the progress of the model during training and can help identify any issues with the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imK6Fix504dY"},"outputs":[],"source":["def save_sample(epoch, iter):\n","    \"\"\"\n","    Save stylized samples from the model to disk.\n","\n","    This function saves stylized samples obtained by applying the trained model\n","    to a batch of test style and content images. The stylized samples are saved\n","    in a grid format, including the original content images, style images, and\n","    the stylized output.\n","\n","    Args:\n","        epoch (int): The current epoch number of the training process.\n","        iter (int): The current iteration number within the epoch.\n","\n","    Returns:\n","        None: The function saves the images to disk and does not return any value.\n","\n","    Example:\n","        for epoch in range(num_epochs):\n","            for i, (style, content) in enumerate(zip(test_style_dl, test_content_dl), 1):\n","                save_sample(epoch, i)\n","    \"\"\"\n","    for (i, (style, content)) in enumerate(zip(test_style_dl, test_content_dl), 1):\n","        with torch.no_grad():\n","            # Generate stylized output using the model\n","            out = model.generate(content[0], style[0])\n","\n","        # Denormalize the images before saving\n","        content = denorm(content[0], device)\n","        style = denorm(style[0], device)\n","        out = denorm(out, device)\n","\n","        # Resize the images to a new height and width\n","        content = rez(content, size=(new_height, new_width))\n","        style = rez(style, size=(new_height, new_width))\n","        out = rez(out, size=(new_height, new_width))\n","\n","        # Concatenate the images into a single grid\n","        res = torch.cat([content, style, out], dim=0)\n","        res = res.to('cpu')\n","\n","        # Save the grid of stylized samples to disk\n","        torchvision.utils.save_image(res, f'/home/ec2-user/SageMaker/test-images/{epoch}_epoch_{iter}_iteration_{i}_content.png', nrow=batch_size)"]},{"cell_type":"markdown","metadata":{"id":"5BhOd3iRUwoC"},"source":["# Learning Rate Scheduler"]},{"cell_type":"markdown","metadata":{"id":"KEKG45VWyfu-"},"source":["This function adjusts the learning rate of the optimizer based on the number of iterations completed and a specified learning rate decay. The learning rate is decreased as the number of iterations increases, which helps the optimizer converge more effectively as it approaches the optimal solution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oobu9EgTy7mc"},"outputs":[],"source":["def adjust_learning_rate(optimiser, iters, learning_rate_decay):\n","    \"\"\"\n","    Adjusts the learning rate of the optimizer during training based on the number of iterations.\n","\n","    Args:\n","        optimiser (torch.optim.Optimizer): The optimizer whose learning rate needs to be adjusted.\n","        iters (int): The current number of iterations in the training process.\n","        learning_rate_decay (float): The learning rate decay factor, controlling the rate of learning rate reduction.\n","\n","    Returns:\n","        None: The function modifies the learning rate in-place within the optimizer.\n","\n","    Example:\n","        optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n","        for epoch in range(num_epochs):\n","            for i, (inputs, labels) in enumerate(train_loader):\n","                # Perform training steps here\n","                iters = epoch * len(train_loader) + i\n","                adjust_learning_rate(optimiser, iters, learning_rate_decay=0.001)\n","    \"\"\"\n","    optimiser.param_groups[0]['lr'] = 0.001 / (1.0 + learning_rate_decay * iters)\n"]},{"cell_type":"markdown","metadata":{"id":"Bc39Al0D4lmX"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"BEDplahfCTqZ"},"source":["The function takes arguments such as the number of epochs to train for, the optimizer to use, and the directory to save the trained model file. It also includes options for changing the number of iterations which is useful when resuming training, learning rate decay, and saving intermediate samples and model checkpoints.\n","\n","The try block contains the code that may raise an exception, which in this case is caused by a truncated image file, PIL.Image.DecompressionBombError which may occur when reading and loading image files, and a StopIteration exception when there are no more style images in the dataset.\n","\n","The except block catches the exception and performs an appropriate action. In the case of a truncated image file the error message will be printed, and the current batch of data will be skipped. If a StopIteration error is encountered, the style dataset iterator will be reset to the beginning, and training will continue with the next batch of content data.\n","\n","The learning rate is decayed over time to improve convergence. The function prints the loss after every 500 iterations and saves the model checkpoint after every 1000 iterations. Finally, the function saves intermediate samples at the end of each epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlztrZ5hX7TY"},"outputs":[],"source":["def fit(epochs, optimizer=torch.optim.SGD, model_state_dir='/home/ec2-user/SageMaker/trained-models', iters=1,\n","        epc=1, learning_rate_decay=5e-5):\n","    \"\"\"\n","    Train the model for a specified number of epochs.\n","\n","    This function performs the training loop for a given number of epochs and updates the model's parameters\n","    using the provided optimizer. The training progress is printed for each iteration, and the loss is recorded\n","    in a list. Additionally, the function saves the model state and stylized samples at specific intervals.\n","\n","    Args:\n","        epochs (int): The total number of epochs to train the model.\n","        optimizer (torch.optim.Optimizer, optional): The optimizer to update model parameters. Default is SGD.\n","        model_state_dir (str, optional): Directory to save the trained model's state. Default is '/home/ec2-user/SageMaker/trained-models'.\n","        iters (int, optional): The initial value for the number of iterations. Default is 1.\n","        epc (int, optional): The initial value for the epoch number. Default is 1.\n","        learning_rate_decay (float, optional): The learning rate decay factor for adjusting the learning rate during training. Default is 5e-5.\n","\n","    Returns:\n","        list: List of recorded losses during the training process.\n","\n","    Example:\n","        # Assuming you have a defined model, content_dl, and style_dl\n","        optim = torch.optim.Adam(model.parameters(), lr=0.001)\n","        loss_list = fit(epochs=20, optimizer=optim)\n","    \"\"\"\n","    # Initialize an empty list to record losses during training\n","    loss_list = []\n","\n","    # Get the total number of iterations in the content dataloader\n","    iterations = len(content_dl)\n","\n","    # Training loop for each epoch\n","    for e in range(1, epochs + 1):\n","        print(f'Start {e} epoch')\n","\n","        # Create an iterator for the style dataloader\n","        style_iter = iter(style_dl)\n","         # start training\n","        # Iterate over content images in the dataloader\n","        for (i, content) in tqdm(enumerate(content_dl, 1)):\n","            try:\n","                # Fetch a batch of style images from the style dataloader\n","                style = next(style_iter)\n","\n","                # Calculate the loss and backpropagate to update model parameters\n","                loss = model(content[0], style[0])\n","                loss_list.append(loss.item())\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","\n","                # Decay the Learning rate after each iteration\n","                adjust_learning_rate(optimizer, iters, learning_rate_decay)\n","\n","            except (OSError, PIL.Image.DecompressionBombError) as e:\n","                print(f\"Skipping batch due to truncated image file.\")\n","                continue\n","            except StopIteration as e:\n","                # Handle the case when the style iterator reaches the end, reset it for the next epoch\n","                style_iter = iter(style_dl)\n","                continue\n","\n","            # Printing the loss for every 500 iterations\n","            if iters % 500 == 0:\n","                 print(f'[{epc}/total 20 epoch],[{i} /'\n","                  f'total {round(iterations)} iteration]: {loss.item()}')\n","\n","            # Saving the model state every 1000 iterations\n","            if iters % 1000 == 0:\n","                torch.save(model.state_dict(), f'{model_state_dir}/{epc}_epoch_{iters}_iterations.pth')\n","\n","            iters = iters + 1\n","\n","        # Saving the stylized samples at the end of each epoch\n","        save_sample(epc, iters)\n","        epc = epc + 1\n","\n","    return loss_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnfKq9bpX7QN"},"outputs":[],"source":["def modelbase(reuse, learning_rate):\n","    \"\"\"\n","    Create a new model and optimizer or reuse an existing model with a specified learning rate for resuming training.\n","\n","    This function is used to initialize a new instance of the model and optimizer or load an existing model's state\n","    using the provided file path. The optimizer is created with the given learning rate.\n","\n","    Args:\n","        reuse (str or None): File path to the saved state of an existing model to be reused, or None to create a new model.\n","        learning_rate (float): The learning rate for the optimizer.\n","\n","    Returns:\n","        tuple: A tuple containing the model and optimizer instances.\n","\n","    Example:\n","        # Create a new model and optimizer\n","        model, optimizer = modelbase(reuse=None, learning_rate=0.001)\n","\n","        # Reuse an existing model and specify a different learning rate\n","        model, optimizer = modelbase(reuse='/path/to/existing_model.pth', learning_rate=0.0005)\n","    \"\"\"\n","    # Create a new instance of the model and move it to the appropriate device\n","    model = to_device(Model(), device)\n","\n","    # If 'reuse' is provided, load the model's state from the specified file path\n","    if reuse is not None:\n","        model.load_state_dict(torch.load(reuse))\n","\n","    # Create the optimizer with the specified learning rate and model parameters\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Return the model and optimizer instances as a tuple\n","    return model, optimizer"]},{"cell_type":"markdown","metadata":{"id":"8-8qpQ5D-sFW"},"source":["#Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1--aDGeUgg0"},"outputs":[],"source":["model,optimizer = modelbase(reuse=None,learning_rate= 0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTxi5KQ3x6a7"},"outputs":[],"source":["epochs = 20"]},{"cell_type":"markdown","metadata":{"id":"20WmLfzf-VKC"},"source":["Please note that the errors in the code cells presented below have been rectified."]},{"cell_type":"markdown","metadata":{"id":"tYYbjsKQ-oUD"},"source":["### Training 1st part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8TU_boONyYgh","outputId":"07beec38-ca34-42b2-a3dd-44e1d1c91406","scrolled":true},"outputs":[],"source":["history = fit(epochs,\n","              optimizer)"]},{"cell_type":"markdown","metadata":{"id":"c7gOLUw8-1v6"},"source":["### Training 2nd part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ3nW81ylERY","outputId":"3f1c29be-6519-47a5-b2c3-d1ea1c24ec1d"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters = 857 )"]},{"cell_type":"markdown","metadata":{"id":"48dHP5VY-5ni"},"source":["### Training 3rd part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_77vLg6XlERY","outputId":"ac1d0896-0015-4514-a6b6-f7bec5dff7ed"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters = 1681)"]},{"cell_type":"markdown","metadata":{"id":"bPP2kJHm-8mT"},"source":["### Training 4th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFryE0tzlERY","outputId":"979e6615-e0d0-432c-e0ad-101ea8dda27d"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters = 1822)"]},{"cell_type":"markdown","metadata":{"id":"6XFNg4_y-_Vh"},"source":["### Training 5th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tTo9nvKlERY","outputId":"8973796f-84b4-481d-97f3-7c2a386e3230"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters = 2622)"]},{"cell_type":"markdown","metadata":{"id":"T0XmjMpc_CGU"},"source":["### Training 6th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfo1DBPflERZ","outputId":"ff6917e8-ede2-4bd6-e743-aad524234e0f"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters = 3251)"]},{"cell_type":"markdown","metadata":{"id":"0PPP-Rdw_FeF"},"source":["### Training 7th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJujQXfIlERU","outputId":"10d26a47-05ea-40a4-fa65-426234ad2766"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters=3350)"]},{"cell_type":"markdown","metadata":{"id":"S7Ji9ddx_IQi"},"source":["### Training 8th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sN_H9X5lERV","outputId":"a417d354-202a-413c-b2bf-60ef37bd98d4"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters=4490)"]},{"cell_type":"markdown","metadata":{"id":"Ur3kJOgL_MP7"},"source":["### Training 9th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nh-XoDA7lERV","outputId":"375ac0f5-8c51-47ec-d9a8-5ca46f78a3d7"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters=4500)"]},{"cell_type":"markdown","metadata":{"id":"sUMH-Zgs_QGz"},"source":["### Training 10th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fjbp0c6PlERW","outputId":"a6489dcb-83da-4ec6-dd3c-5b6171940ba4"},"outputs":[],"source":["history = fit(epochs,\n","              optimizer,iters=5001)"]},{"cell_type":"markdown","metadata":{"id":"ykMC5BSC_TkN"},"source":["### Training 11th part."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TDNhFDP0oqH"},"outputs":[],"source":["model,optimizer = modelbase(reuse='/home/ec2-user/SageMaker/trained-models/1_epoch_15000_iterations.pth',learning_rate= 0.00068)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"282Yi_UqlERW","outputId":"5a2111be-885f-40c1-d476-f6c3613d130c"},"outputs":[],"source":["epochs =19\n","\n","history = fit(epochs,\n","              optimizer,iters=11594,epc=2)"]},{"cell_type":"markdown","metadata":{"id":"85BvC_lz4MKo"},"source":["## Model training last part"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al5sk_2ulERX","outputId":"0792bfd8-8ce2-4aae-ab6a-be3b39935951"},"outputs":[],"source":["model,optimizer = modelbase(reuse='/home/ec2-user/SageMaker/trained-models/11_epoch_107000_iterations.pth',\n","                            learning_rate= 0.0002)\n","\n","epochs = 10\n","history = fit(epochs,\n","              optimizer,iters=107001,epc=11)"]},{"cell_type":"markdown","metadata":{"id":"c5ZeQzp84R9u"},"source":["Saving the final trained model, creating a zip of all the trained models that are saved every 1000 iterations and cretaing a zip of all the test image files that have been saved after every epoch.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4B6loVVzzxB"},"outputs":[],"source":["torch.save(model.state_dict(), '/home/ec2-user/SageMaker/final.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oitbTY3XlERW","outputId":"605911b7-098f-49b7-b57c-01f9dfb0c673"},"outputs":[],"source":["shutil.make_archive('/home/ec2-user/SageMaker/models', 'zip','/home/ec2-user/SageMaker/trained-models')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZ7EBtVZlERW","outputId":"4797131e-de6b-47d9-f7da-e554e6b94f12"},"outputs":[],"source":["shutil.make_archive('/home/ec2-user/SageMaker/testimages', 'zip','/home/ec2-user/SageMaker/test-images')"]},{"cell_type":"markdown","metadata":{"id":"shzbOoHdUhIi"},"source":["# Plotting The loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOEP0GpeX7NY"},"outputs":[],"source":["def loss_plot(history):\n","    \"\"\"\n","    Plots the training loss over iterations.\n","\n","    Parameters:\n","        history (list): A list containing the training loss values over iterations.\n","\n","    Returns:\n","        None\n","\n","    Example:\n","        loss_values = [0.5, 0.4, 0.3, 0.2, 0.1]\n","        loss_plot(loss_values)\n","    \"\"\"\n","    # Create a line plot of training loss values over iterations\n","    plt.plot(range(len(history)), history)\n","\n","    # Label the x-axis as 'iteration'\n","    plt.xlabel('iteration')\n","\n","    # Label the y-axis as 'loss'\n","    plt.ylabel('loss')\n","\n","    # Set the title of the plot as 'train loss'\n","    plt.title('train loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GfSXsSyUlERZ","outputId":"9a40b4e9-9af8-4b2a-e506-8a8c05d1d99e"},"outputs":[],"source":["loss_plot(history)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02e38fc8863f4a07b9c5c5f088cf50ea":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcb03f09d124ccc9fdaf56fa31b2fb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e8b0d63c57a4f3f825ea905f962df8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bcb03f09d124ccc9fdaf56fa31b2fb6","placeholder":"​","style":"IPY_MODEL_6b3b21181c7447cbbf8e59a40af56ee7","value":"100%"}},"49321942e186447fb8ef3b807c996d75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b3b21181c7447cbbf8e59a40af56ee7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e56006fdbbd40b2a2ec2ebbaae9fd11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ece4908ecff4bf9ab54a48e156db713":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"747ed4e30ae94c40aa217228430c0cbc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3e8b0d63c57a4f3f825ea905f962df8d","IPY_MODEL_b16ebe22c0c147e983324cf354dbe730","IPY_MODEL_b52bea844f0644c7aa760a9856d458fe"],"layout":"IPY_MODEL_02e38fc8863f4a07b9c5c5f088cf50ea"}},"b16ebe22c0c147e983324cf354dbe730":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5c7496780814ad5aba072790591e1bd","max":574673361,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ece4908ecff4bf9ab54a48e156db713","value":574673361}},"b52bea844f0644c7aa760a9856d458fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49321942e186447fb8ef3b807c996d75","placeholder":"​","style":"IPY_MODEL_6e56006fdbbd40b2a2ec2ebbaae9fd11","value":" 548M/548M [00:07&lt;00:00, 80.8MB/s]"}},"c5c7496780814ad5aba072790591e1bd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
